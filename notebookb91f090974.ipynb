{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoModel, AutoProcessor, AutoTokenizer\nimport torch\nMODEL_JINA_NAME = \"jinaai/jina-clip-v2\"\nHUGGINGFACE_CACHE_DIR = \"./cache\"\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.device_count() > 1:\n    print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n\n# Adjust the processor to resize images to (224, 224)\nprocessor_jina = AutoProcessor.from_pretrained(\n    MODEL_JINA_NAME,\n    cache_dir=HUGGINGFACE_CACHE_DIR,\n    trust_remote_code=True\n)\n\n# Custom processing to resize input images\nprocessor_jina.feature_extractor.size = (224, 224)\n\n# The model setup remains the same\nmodel_jina = AutoModel.from_pretrained(\n    MODEL_JINA_NAME,\n    cache_dir=HUGGINGFACE_CACHE_DIR,\n    trust_remote_code=True,\n    attn_implementation=\"eager\"\n).to('cpu').float()\n\ntokenizer_jina = AutoTokenizer.from_pretrained(\n    MODEL_JINA_NAME,\n    cache_dir=HUGGINGFACE_CACHE_DIR,\n    trust_remote_code=True,\n    use_fast=True\n)\n\n\n# !git clone https://github.com/openai/CLIP.git\n# #https://github.com/openai/CLIP\n# #CLIP (Contrastive Language-Image Pre-Training)\n# #Learning Transferable Visual Models From Natural Language Supervision\n# #Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\n# #Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\n\n# !git clone https://github.com/CompVis/taming-transformers\n\n# #https://github.com/CompVis/taming-transformers\n# #Taming Transformers for High-Resolution Image Synthesis\n# #Patrick Esser, Robin Rombach, Bj√∂rn Ommer","metadata":{"_uuid":"aebb64a2-3436-4b41-9348-a4d4d0e38272","_cell_guid":"1205aa0d-9491-4657-a36f-bb36bf490473","trusted":true,"collapsed":false,"id":"Za86vmBb0bVT","outputId":"c644a74c-fd49-4cc0-e022-c43facb594b9","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import nn\n# Wrap the model in DataParallel for multi-GPU usage\nmodel_jina = nn.DataParallel(model_jina, device_ids=[0, 1])  # Specify GPUs 0 and 1","metadata":{"_uuid":"c5f03606-3bb1-4e03-9ba7-2733ea3c41bf","_cell_guid":"6beb05d4-74d1-413e-9d05-518f2a1327f1","trusted":true,"collapsed":false,"id":"uY8i3Kt4jvY2","outputId":"b0d17f1a-330f-46fc-9545-dbb998ba1c08","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"cb93f215-17f0-4e4b-9716-8f1c835649a3","_cell_guid":"bd9f154c-0635-462a-8ef7-cb779e695369","trusted":true,"collapsed":false,"id":"8wofcATFhlOn","outputId":"3b655fea-8a75-4580-9e9c-0c54106e58e0","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## install some extra libraries\n# !pip install --no-deps ftfy regex tqdm\n# !pip install omegaconf==2.0.0 pytorch-lightning==1.0.8\n# !pip uninstall torchtext --yes\n# !pip install einops","metadata":{"_uuid":"7c86ecd8-35ab-437a-a581-0d963a045bf5","_cell_guid":"22ebe517-0ee8-4c93-bedc-d6a621caf3e2","trusted":true,"collapsed":false,"id":"ompo3S_w7Ej6","outputId":"6ea4d69b-b399-4c69-85f7-217af1603871","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"_uuid":"e71c62de-d51c-4822-8afb-573d3f349e90","_cell_guid":"f396f111-0c03-4ed5-a33c-eccf1acdc451","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# import libraries\nimport numpy as np\nimport os, imageio, pdb, math\nimport torchvision\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\n\nimport PIL\nimport matplotlib.pyplot as plt\n\nimport yaml\nfrom omegaconf import OmegaConf\n\n# from CLIP import clip\n\n#import warnings\n#warnings.filterwarnings('ignore')","metadata":{"_uuid":"d4edbf87-e719-4b80-9d5e-721288091997","_cell_guid":"2db6d7bf-f7f3-4c02-a21a-836b86d78bbc","trusted":true,"collapsed":false,"id":"Zn_gTOEw7h8x","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## helper functions\n\ndef show_from_tensor(tensor):\n  img = tensor.clone()\n  img = img.mul(255).byte()\n  img = img.cpu().numpy().transpose((1,2,0))\n\n  plt.figure(figsize=(10,7))\n  plt.axis('off')\n  plt.imshow(img)\n  plt.show()\n\ndef norm_data(data):\n  return (data.clip(-1,1)+1)/2 ### range between 0 and 1 in the result\n\n### Parameters\nlearning_rate = .5\nbatch_size = 1\nwd = .1\nnoise_factor = .22\n\ntotal_iter=400\nim_shape = [450, 450, 3] # height, width, channel\nsize1, size2, channels = im_shape","metadata":{"_uuid":"ca2fddcb-c697-4dea-a9fa-7c82fcd2751a","_cell_guid":"f69901f6-81b5-4f92-ade2-c0491bd16f21","trusted":true,"collapsed":false,"id":"pZehdKop7_bK","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### CLIP MODEL ###\n# clipmodel, _ = clip.load('ViT-B/32', jit=False)\nclipmodel = model_jina\nclipmodel.eval()\n# print(clip.available_models())\nprint(dir(clipmodel))\n# print(\"Clip model visual input resolution: \", clipmodel.visual.input_resolution)\n\n# device=torch.device(\"cuda:0\")\ntorch.cuda.empty_cache()","metadata":{"_uuid":"72c1b9b9-6e4b-44d2-8aaf-3c4e5d73ff75","_cell_guid":"d53f03f1-7951-4ccd-a380-8bfc93b72a27","trusted":true,"collapsed":false,"id":"JfApIAoR-N55","outputId":"e0b5609b-4021-41fe-c17b-02e0eac94241","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls","metadata":{"_uuid":"b7303e44-e1b0-4412-b60a-c9d1e73419fc","_cell_guid":"02f14ac1-3b03-498c-bf3c-4a425b1a2129","trusted":true,"collapsed":false,"id":"ow5gAnmVhNuT","outputId":"e24dbe37-7cd1-4e2a-d545-b4026e2253e4","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Taming transformer instantiation\n\n%cd taming-transformers/\n\n!mkdir -p models/vqgan_imagenet_f16_16384/checkpoints\n!mkdir -p models/vqgan_imagenet_f16_16384/configs\n\nif len(os.listdir('models/vqgan_imagenet_f16_16384/checkpoints/')) == 0:\n   !wget 'https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1' -O 'models/vqgan_imagenet_f16_16384/checkpoints/last.ckpt'\n   !wget 'https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1' -O 'models/vqgan_imagenet_f16_16384/configs/model.yaml'","metadata":{"_uuid":"9c30cdf8-0d81-4d46-92ff-90f4e6314a5d","_cell_guid":"758840bf-6a99-4e87-b37f-24a15386c009","trusted":true,"collapsed":false,"id":"C_MJ3iNdAW7Z","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !curl -o utils.py https://raw.githubusercontent.com/calinjovrea-imvision/utils-jina/refs/heads/main/utils.py\n# !mv utils.py taming/data/utils.py\n# !cat taming/data/utils.py\n# %cd taming-transformers/","metadata":{"_uuid":"1c2ce3e2-7c9c-43ca-a02c-f8e6baaf2433","_cell_guid":"bd6bdbe9-ce7f-472a-857e-8024c50d6134","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# \nfrom taming.models.vqgan import VQModel\n\ndef load_config(config_path, display=False):\n   config_data = OmegaConf.load(config_path)\n   if display:\n     print(yaml.dump(OmegaConf.to_container(config_data)))\n   return config_data\n\ndef load_vqgan(config, chk_path=None):\n  model = VQModel(**config.model.params)\n  if chk_path is not None:\n    state_dict = torch.load(chk_path, map_location=\"cpu\")[\"state_dict\"]\n    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n  return model.eval()\n\ndef generator(x):\n  x = taming_model.post_quant_conv(x)\n  x = taming_model.decoder(x)\n  return x\n\ntaming_config = load_config(\"./models/vqgan_imagenet_f16_16384/configs/model.yaml\", display=True)\ntaming_model = load_vqgan(taming_config, chk_path=\"./models/vqgan_imagenet_f16_16384/checkpoints/last.ckpt\").to(device)","metadata":{"_uuid":"e9bd27e4-4075-40df-9bf1-acc7021735dc","_cell_guid":"8fdccbe0-ffa7-400c-b15d-eccb0be845b7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"1000b2e9-f24a-43ea-be1a-bf98d9923739","_cell_guid":"b0adde60-ba40-4f8d-8b7f-8daefcad92fd","trusted":true,"collapsed":false,"id":"ftDHxwo_B2F_","outputId":"8bff8943-915e-4017-f56e-f738e97e2405","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Declare the values that we are going to optimize\n\nclass Parameters(torch.nn.Module):\n  def __init__(self):\n    super(Parameters, self).__init__()\n    self.data = .5*torch.randn(batch_size, 256, size1//16, size2//16).cuda() # 1x256x14x15 (225/16, 400/16)\n    self.data = torch.nn.Parameter(torch.sin(self.data))\n\n  def forward(self):\n    return self.data\n\ndef init_params():\n  params=Parameters().cuda()\n  optimizer = torch.optim.AdamW([{'params':[params.data], 'lr': learning_rate}], weight_decay=wd)\n  return params, optimizer","metadata":{"_uuid":"0a96784d-de6f-4135-8586-de2b1a512822","_cell_guid":"889df354-1071-4d6c-b91b-049fc36c1aa9","trusted":true,"collapsed":false,"id":"hY6dOXE7Drgn","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Encoding prompts and a few more things\nnormalize = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n\ndef encodeText(text):\n  t=tokenizer_jina.tokenize(text)\n  t=model_jina.module.encode_text(t)\n  return t\n\ndef createEncodings(include, exclude, extras):\n  include_enc=[]\n  for text in include:\n    include_enc.append(encodeText(text))\n  exclude_enc=encodeText(exclude) if exclude != '' else 0\n  extras_enc=encodeText(extras) if extras !='' else 0\n\n  return include_enc, exclude_enc, extras_enc\n\naugTransform = torch.nn.Sequential(\n    torchvision.transforms.RandomHorizontalFlip(),\n    torchvision.transforms.RandomAffine(30, (.2, .2), fill=0)\n).cuda()\n\nParams, optimizer = init_params()\n\nwith torch.no_grad():\n  print(Params().shape)\n  img= norm_data(generator(Params()).cpu()) # 1 x 3 x 224 x 400 [225 x 400]\n  print(\"img dimensions: \",img.shape)\n  show_from_tensor(img[0])","metadata":{"_uuid":"c4a90557-bc9f-4a7c-a854-ff0ca31b2086","_cell_guid":"acf193f6-2712-4d35-b33b-c66509e36e38","trusted":true,"collapsed":false,"id":"BsMOgdHPGFI8","outputId":"40b24266-8907-4ba3-e3ec-46f85389841d","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### create crops\n\ndef create_crops(img, num_crops=32):\n  p=size1//2\n  img = torch.nn.functional.pad(img, (p,p,p,p), mode='constant', value=0) # 1 x 3 x 448 x 624 (adding 112*2 on all sides to 224x400)\n\n  img = augTransform(img) #RandomHorizontalFlip and RandomAffine\n\n  crop_set = []\n  for ch in range(num_crops):\n    gap1= int(torch.normal(1.2, .3, ()).clip(.43, 1.9) * size1)\n    offsetx = torch.randint(0, int(size1*2-gap1),())\n    offsety = torch.randint(0, int(size1*2-gap1),())\n\n    crop=img[:,:,offsetx:offsetx+gap1, offsety:offsety+gap1]\n\n    crop = torch.nn.functional.interpolate(crop,(224,224), mode='bilinear', align_corners=True)\n    crop_set.append(crop)\n\n  img_crops=torch.cat(crop_set,0) ## 30 x 3 x 224 x 224\n\n  randnormal = torch.randn_like(img_crops, requires_grad=False)\n  num_rands=0\n  randstotal=torch.rand((img_crops.shape[0],1,1,1)).cuda() #32\n\n  for ns in range(num_rands):\n    randstotal*=torch.rand((img_crops.shape[0],1,1,1)).cuda()\n\n  img_crops = img_crops + noise_factor*randstotal*randnormal\n\n  return img_crops","metadata":{"_uuid":"bdd88fd7-7556-4320-aea4-a76b1cfed301","_cell_guid":"9170d4f3-0334-477a-b4b9-1859b1162869","trusted":true,"collapsed":false,"id":"XsgmO1LeI_5e","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Show current state of generation\n\ndef showme(Params, show_crop):\n  with torch.no_grad():\n    generated = generator(Params())\n\n    if (show_crop):\n      print(\"Augmented cropped example\")\n      aug_gen = generated.float() # 1 x 3 x 224 x 400\n      aug_gen = create_crops(aug_gen, num_crops=1)\n      aug_gen_norm = norm_data(aug_gen[0])\n      show_from_tensor(aug_gen_norm)\n\n    print(\"Generation\")\n    latest_gen=norm_data(generated.cpu()) # 1 x 3 x 224 x 400\n    show_from_tensor(latest_gen[0])\n\n  return (latest_gen[0])","metadata":{"_uuid":"c077b5b5-178d-417f-b3ab-69a654e54023","_cell_guid":"876eae48-6f3a-41c0-93ed-570a69f6b7eb","trusted":true,"collapsed":false,"id":"lwESrZefM0Vt","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optimization process\nfrom PIL import Image\nimport numpy as np\n\ndef optimize_result(Params, prompt):\n  alpha=1 ## the importance of the include encodings\n  beta=.5 ## the importance of the exclude encodings\n\n  ## image encoding\n  # out = generator(Params())\n  # out = norm_data(out)\n  # out = create_crops(out)\n  # out = normalize(out) # 30 x 3 x 224 x 224\n  # out = out.cpu().detach().numpy()[0][0]\n  # print(out.shape)\n  # out = np.clip(out, 0, 255).astype(np.uint8)\n  # out = Image.fromarray(out)\n  # print(out)\n  # image_enc=clipmodel.encode_image(out) ## 30 x 512\n  # print(image_enc)\n  # print(type(image_enc))\n  # print(image_enc.shape)\n  # ## text encoding  w1 and w2\n  # final_enc = w1*prompt + w1*extras_enc # prompt and extras_enc : 1 x 512\n  # final_text_include_enc = final_enc / final_enc.norm(dim=-1, keepdim=True) # 1 x 512\n  # final_text_exclude_enc = exclude_enc\n\n  # Image encoding pipeline\n  out = generator(Params())\n  out = norm_data(out)\n  out = create_crops(out)\n  out = normalize(out)  # Tensor: shape (30, 3, 224, 224)\n\n  print(out.shape)\n  # Optionally move to CPU if required, otherwise keep on GPU\n  # out = out[0, 0]  # Selecting the first crop (for simplicity)\n\n  # # Convert the tensor to uint8 format and clip values between 0 and 255\n  # out = torch.clamp(out, 0, 255).to(torch.uint8)\n\n  # print(out.shape)\n\n  # # Convert the tensor to a PIL Image for display or further processing\n  # image_out = Image.fromarray(out.permute(1, 2, 0).cpu().numpy())\n\n  # # Encode the image with clipmodel\n  # image_enc = clipmodel.encode_image(out.unsqueeze(0))  # (1, 512)\n\n  # out = torch.clamp(out, 0, 255).to(torch.uint8)\n  # print(out.shape)\n  # Convert the 2D tensor to a NumPy array (height x width)\n  # out_np = out.cpu().numpy()\n\n  # Convert to PIL Image\n  # image_out = Image.fromarray(out_np)\n\n  # Display the image (optional)\n  # image_out.show()\n\n  # Encode the image with clipmodel (if needed)\n  # image_enc = clipmodel.encode_image(image_out)\n\n  # Print out the encoding shape for verification\n  # Assuming `out` has the shape (32, 3, 224, 224)\n  out = torch.clamp(out, 0, 255).to(torch.uint8)  # Clamp values to valid range for images\n  print(out.shape)  # Should print torch.Size([32, 3, 224, 224])\n\n  # Move to CPU and convert to NumPy (if needed for PIL or processing)\n  out_np = out.permute(0, 2, 3, 1).cpu().numpy()  # Convert to (32, 224, 224, 3) format for PIL\n\n  # Convert each image to PIL and encode all 32 images\n  image_encodings = []\n  for i in range(out_np.shape[0]):  # Iterate through all 32 images\n      # print(out_np[i].shape)\n      # img_np = out[i].permute(1, 2, 0).cpu().numpy()  # Convert CHW to HWC\n      img_pil = Image.fromarray(out_np[i], mode=\"RGB\")\n      # image_out = Image.fromarray(out_np[i])  # Convert the ith image to PIL format\n      #image_enc = model_jina.encode_image(img_pil)  # Encode ith image\n      image_encodings.append(img_pil)\n\n  inputs = processor_jina(images=image_encodings, return_tensors=\"pt\", padding=True)\n\n  inputs = inputs.to('cpu')\n  print(type(inputs))\n  # print(inputs)\n  # Encode the batch\n  with torch.no_grad():\n      embeddings = model_jina.module.get_image_features(**inputs)\n    \n  print(embeddings.shape)  # Check shape of output embedding\n\n  # Stack all image encodings into a single tensor\n  # image_encodings = torch.stack(embeddings, dim=0)\n  print(\"Image encoding shape:\", embeddings.shape)\n\n  final_embedding = embeddings.mean(dim=0)  # Shape: (1024)\n  print(f\"final_embedding {final_embedding.shape}\")\n\n  # Text encoding with w1 and w2\n  final_enc = w1 * prompt + w1 * extras_enc  # (1, 512)\n\n  final_enc = torch.tensor(final_enc, requires_grad=True)  # Convert NumPy array to tensor if needed\n\n  # Normalize the final text encoding\n  final_text_include_enc = final_enc / final_enc.norm(dim=-1, keepdim=True)  # (1, 512)\n  final_text_exclude_enc = exclude_enc  # Assuming exclude_enc is already a tensor\n\n  # image_enc = torch.tensor(image_enc)\n  final_text_exclude_enc = torch.tensor(final_text_exclude_enc)\n\n  ## calculate the loss\n  main_loss = torch.cosine_similarity(final_text_include_enc, final_embedding, -1) # 30\n  penalize_loss = torch.cosine_similarity(final_text_exclude_enc, final_embedding, -1) # 30\n\n  final_loss = -alpha*main_loss.mean() + beta*penalize_loss.mean()\n\n  return final_loss\n\ndef optimize(Params, optimizer, prompt):\n  loss = optimize_result(Params, prompt).mean()\n  optimizer.zero_grad()\n  loss.backward()\n  optimizer.step()\n  return loss","metadata":{"_uuid":"62d4ecd7-b1e6-4c81-9ebf-fee241bbb569","_cell_guid":"52c519a6-d3a8-44a2-8690-1a7c2f87e1d1","trusted":true,"collapsed":false,"id":"6uV4VJn7N-FI","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### training loop\n\ndef training_loop(Params, optimizer, show_crop=False):\n  res_img=[]\n  res_z=[]\n\n  for prompt in include_enc:\n    iteration=0\n    Params, optimizer = init_params() # 1 x 256 x 14 x 25 (225/16, 400/16)\n\n    for it in range(total_iter):\n      loss = optimize(Params, optimizer, prompt)\n\n      if iteration>=80 and iteration%show_step == 0:\n        new_img = showme(Params, show_crop)\n        res_img.append(new_img)\n        res_z.append(Params()) # 1 x 256 x 14 x 25\n        print(\"loss:\", loss.item(), \"\\niteration:\",iteration)\n\n      iteration+=1\n    torch.cuda.empty_cache()\n  return res_img, res_z","metadata":{"_uuid":"25ae6c6a-730d-49e9-828b-6066f26e630c","_cell_guid":"12651de4-9a1b-46f4-b238-c4b4b9540cff","trusted":true,"collapsed":false,"id":"9vpVuN8iQuLb","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()\ninclude=['sketch of a lady', 'sketch of a man on a horse']\n# include=['A painting of a pineapple in a bowl']\nexclude='watermark'\nextras = \"\"\nw1=1\nw2=1\nnoise_factor= .22\ntotal_iter=110\nshow_step=10 # set this to see the result every 10 interations beyond iteration 80\ninclude_enc, exclude_enc, extras_enc = createEncodings(include, exclude, extras)\nres_img, res_z=training_loop(Params, optimizer, show_crop=True)","metadata":{"_uuid":"a9e55d34-1528-42b2-9806-918920598738","_cell_guid":"fbb8aa95-a070-41be-84ba-c15079d5e9bd","trusted":true,"collapsed":false,"id":"9q4kwgudSuI5","outputId":"45e757e4-0c2a-4839-aeb7-f711cba38021","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(res_img), len(res_z))\nprint(res_img[0].shape, res_z[0].shape)\nprint(res_z[0].max(), res_z[0].min())","metadata":{"_uuid":"f8cd789b-d055-49dc-b488-bd910630b91c","_cell_guid":"58d4f31a-caea-4c48-a538-4b135655ad2c","trusted":true,"collapsed":false,"id":"ScpytmgQUffY","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()\ninclude=['A forest with purple trees', 'an elephant at the top of a mountain, looking at the stars','one hundred people with green jackets']\nexclude='watermark, cropped, confusing, incoherent, cut, blurry'\nextras = \"\"\nw1=1\nw2=1\nnoise_factor= .22\ntotal_iter=110\nshow_step=total_iter-1 # set this if you want to interpolate between only the final versions\ninclude_enc, exclude_enc, extras_enc = createEncodings(include, exclude, extras)\nres_img, res_z=training_loop(Params, optimizer, show_crop=False)","metadata":{"_uuid":"25852edb-f869-4c2a-aff2-33bf29bc9af4","_cell_guid":"ec429ee9-d8d4-40c9-afc0-7efe9519254c","trusted":true,"collapsed":false,"id":"W_NnBA9AXaqg","outputId":"a6649c66-cd6c-4570-97ee-196db0e1bee2","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def interpolate(res_z_list, duration_list):\n  gen_img_list=[]\n  fps = 25\n\n  for idx, (z, duration) in enumerate(zip(res_z_list, duration_list)):\n    num_steps = int(duration*fps)\n    z1=z\n    z2=res_z_list[(idx+1)%len(res_z_list)] # 1 x 256 x 14 x 25 (225/16, 400/16)\n\n    for step in range(num_steps):\n      alpha = math.sin(1.5*step/num_steps)**6\n      z_new = alpha * z2 + (1-alpha) * z1\n\n      new_gen=norm_data(generator(z_new).cpu())[0] ## 3 x 224 x 400\n      new_img=T.ToPILImage(mode='RGB')(new_gen)\n      gen_img_list.append(new_img)\n\n  return gen_img_list\n\ndurations=[5,5,5,5,5,5]\ninterp_result_img_list = interpolate(res_z, durations)","metadata":{"_uuid":"f8fe6871-cb9d-4e2a-ad6d-6bc8f395af0a","_cell_guid":"cfafbf72-0caa-4033-b342-ab92b59200f8","trusted":true,"collapsed":false,"id":"c3lTWZaDYBJR","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## create a video\nout_video_path=f\"../video.mp4\"\nwriter = imageio.get_writer(out_video_path, fps=25)\nfor pil_img in interp_result_img_list:\n  img = np.array(pil_img, dtype=np.uint8)\n  writer.append_data(img)\n\nwriter.close()","metadata":{"_uuid":"3a493bef-03c1-4a88-ac03-c1e329b3c80b","_cell_guid":"0eea1351-be30-40fa-bbdb-9f6064816eea","trusted":true,"collapsed":false,"id":"WeSmO7WRasl_","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import HTML\nfrom base64 import b64encode\n\nmp4 = open('../video.mp4','rb').read()\ndata=\"data:video/mp4;base64,\"+b64encode(mp4).decode()\nHTML(\"\"\"<video width=800 controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data)","metadata":{"_uuid":"52e7a2d0-280e-48de-a8e9-2f8fae06e9a4","_cell_guid":"134806fc-d351-474f-85e4-f9e4740bf967","trusted":true,"collapsed":false,"id":"VmlxcyLzbbu_","outputId":"3fcc0859-a71b-4b5f-f15c-8d2f64c64914","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"880d89fc-88e9-47b0-afef-30f169df3179","_cell_guid":"da527e0b-445d-4584-9c26-2504b63ebb2d","trusted":true,"collapsed":false,"id":"8g77m5Urb8YH","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}